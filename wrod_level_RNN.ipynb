{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import labiries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import collections\n",
    "import re\n",
    "import pickle\n",
    "from random import randint\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from six.moves import cPickle\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#load text into memeory\n",
    "def load_text(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn text into clean tokens\n",
    "def clean_text(text):\n",
    "    # replace '--' with a space ' '\n",
    "    text = text.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = text.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tokens\n",
    "def save_text(lines,filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was eating at an Indian restaurant when a homeless man came in and asked the girl behind the counter if they had any food he could have. She told him there was Naan. [me narrating a documentary about narrators] \"I can't hear what they're saying cuz I'm talking\"\n",
      "I was going to organize a space-themed birthday party for my son... ...but I couldn't planet. Telling my daughter garlic is good for you. Good immune system and keeps pests away.Ticks, mosquitos, vampires... men.\n",
      "Squared My dick is X squared and I rooted your mom. Now she's my X I've been going through a really rough period at work this week It's my own fault for swapping my tampax for sand paper.\n",
      "I call my dick fun Because it's what girls just want to have! If I could have dinner with anyone, dead or alive... ...I would choose alive. -B.J. Novak-\n",
      "Be careful when you ROFL! I once heard a joke in a scissor factory... It left me in stitches. Two guys walk into a bar. The third guy ducks.\n",
      "I'll bet Vampire Kiddies enjoy scabs as m\n"
     ]
    }
   ],
   "source": [
    "#load text\n",
    "in_filename = 'short_jokes.txt'\n",
    "text = load_text(in_filename)\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'was', 'eating', 'at', 'an', 'indian', 'restaurant', 'when', 'a', 'homeless', 'man', 'came', 'in', 'and', 'asked', 'the', 'girl', 'behind', 'the', 'counter', 'if', 'they', 'had', 'any', 'food', 'he', 'could', 'have', 'she', 'told', 'him', 'there', 'was', 'naan', 'me', 'narrating', 'a', 'documentary', 'about', 'narrators', 'i', 'cant', 'hear', 'what', 'theyre', 'saying', 'cuz', 'im', 'talking', 'i', 'was', 'going', 'to', 'organize', 'a', 'spacethemed', 'birthday', 'party', 'for', 'my', 'son', 'but', 'i', 'couldnt', 'planet', 'telling', 'my', 'daughter', 'garlic', 'is', 'good', 'for', 'you', 'good', 'immune', 'system', 'and', 'keeps', 'pests', 'awayticks', 'mosquitos', 'vampires', 'men', 'squared', 'my', 'dick', 'is', 'x', 'squared', 'and', 'i', 'rooted', 'your', 'mom', 'now', 'shes', 'my', 'x', 'ive', 'been', 'going', 'through', 'a', 'really', 'rough', 'period', 'at', 'work', 'this', 'week', 'its', 'my', 'own', 'fault', 'for', 'swapping', 'my', 'tampax', 'for', 'sand', 'paper', 'i', 'call', 'my', 'dick', 'fun', 'because', 'its', 'what', 'girls', 'just', 'want', 'to', 'have', 'if', 'i', 'could', 'have', 'dinner', 'with', 'anyone', 'dead', 'or', 'alive', 'i', 'would', 'choose', 'alive', 'bj', 'novak', 'be', 'careful', 'when', 'you', 'rofl', 'i', 'once', 'heard', 'a', 'joke', 'in', 'a', 'scissor', 'factory', 'it', 'left', 'me', 'in', 'stitches', 'two', 'guys', 'walk', 'into', 'a', 'bar', 'the', 'third', 'guy', 'ducks', 'ill', 'bet', 'vampire', 'kiddies', 'enjoy', 'scabs', 'as', 'much', 'as', 'human', 'kids', 'love', 'pudding', 'skin', 'why', 'cant', 'barbie', 'get', 'pregnant', 'because', 'ken']\n",
      "Total Tokens: 158339\n",
      "Unique Tokens: 14124\n"
     ]
    }
   ],
   "source": [
    "#clean text\n",
    "tokens = clean_text(text)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 158288\n"
     ]
    }
   ],
   "source": [
    "#organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length,len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    " \n",
    "# save sequences to file\n",
    "out_filename = 'scifi_sequences.txt'\n",
    "save_text(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_filename = 'scifi_sequences.txt'\n",
    "text = load_text(in_filename)\n",
    "lines = text.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "#vocabulary size\n",
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "x,y = sequences[:,:-1], sequences[:,-1]\n",
    "y = tf.keras.utils.to_categorical(y,num_classes=vocab_size)\n",
    "seq_length = x.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            706200    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14124)             1426524   \n",
      "=================================================================\n",
      "Total params: 2,283,624\n",
      "Trainable params: 2,283,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size,50,input_length=seq_length))\n",
    "model.add(tf.keras.layers.LSTM(100,return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(100))\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(vocab_size,activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1018 23:18:00.362707 140715490535232 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "158288/158288 [==============================] - 969s 6ms/sample - loss: 6.9889 - acc: 0.0414\n",
      "Epoch 2/30\n",
      "158288/158288 [==============================] - 980s 6ms/sample - loss: 6.6026 - acc: 0.0605\n",
      "Epoch 3/30\n",
      "158288/158288 [==============================] - 980s 6ms/sample - loss: 6.3585 - acc: 0.0748\n",
      "Epoch 4/30\n",
      "158288/158288 [==============================] - 966s 6ms/sample - loss: 6.1656 - acc: 0.0912\n",
      "Epoch 5/30\n",
      "158288/158288 [==============================] - 965s 6ms/sample - loss: 6.0064 - acc: 0.1026\n",
      "Epoch 6/30\n",
      "158288/158288 [==============================] - 952s 6ms/sample - loss: 5.8629 - acc: 0.1104\n",
      "Epoch 7/30\n",
      "158288/158288 [==============================] - 934s 6ms/sample - loss: 5.7278 - acc: 0.1182\n",
      "Epoch 8/30\n",
      "158288/158288 [==============================] - 943s 6ms/sample - loss: 5.6074 - acc: 0.1253\n",
      "Epoch 9/30\n",
      "158288/158288 [==============================] - 941s 6ms/sample - loss: 5.4983 - acc: 0.1316\n",
      "Epoch 10/30\n",
      "158288/158288 [==============================] - 969s 6ms/sample - loss: 5.3970 - acc: 0.1384\n",
      "Epoch 11/30\n",
      "158288/158288 [==============================] - 965s 6ms/sample - loss: 5.3035 - acc: 0.1437\n",
      "Epoch 12/30\n",
      "158288/158288 [==============================] - 942s 6ms/sample - loss: 5.2157 - acc: 0.1497\n",
      "Epoch 13/30\n",
      "158288/158288 [==============================] - 953s 6ms/sample - loss: 5.1281 - acc: 0.1551\n",
      "Epoch 14/30\n",
      "158288/158288 [==============================] - 951s 6ms/sample - loss: 5.0467 - acc: 0.1603\n",
      "Epoch 15/30\n",
      "158288/158288 [==============================] - 951s 6ms/sample - loss: 4.9643 - acc: 0.1652\n",
      "Epoch 16/30\n",
      "158288/158288 [==============================] - 936s 6ms/sample - loss: 4.8820 - acc: 0.1706\n",
      "Epoch 17/30\n",
      "158288/158288 [==============================] - 934s 6ms/sample - loss: 4.8056 - acc: 0.1754\n",
      "Epoch 18/30\n",
      "158288/158288 [==============================] - 857s 5ms/sample - loss: 4.7308 - acc: 0.1800\n",
      "Epoch 19/30\n",
      "158288/158288 [==============================] - 882s 6ms/sample - loss: 4.6646 - acc: 0.1841\n",
      "Epoch 20/30\n",
      "158288/158288 [==============================] - 941s 6ms/sample - loss: 4.5945 - acc: 0.1891\n",
      "Epoch 21/30\n",
      "158288/158288 [==============================] - 930s 6ms/sample - loss: 4.5300 - acc: 0.1938\n",
      "Epoch 22/30\n",
      "158288/158288 [==============================] - 929s 6ms/sample - loss: 4.4631 - acc: 0.1975\n",
      "Epoch 23/30\n",
      "158288/158288 [==============================] - 922s 6ms/sample - loss: 4.3951 - acc: 0.2032\n",
      "Epoch 24/30\n",
      "158288/158288 [==============================] - 936s 6ms/sample - loss: 4.3323 - acc: 0.2077\n",
      "Epoch 25/30\n",
      "158288/158288 [==============================] - 925s 6ms/sample - loss: 4.2707 - acc: 0.2127\n",
      "Epoch 26/30\n",
      "158288/158288 [==============================] - 920s 6ms/sample - loss: 4.2119 - acc: 0.2185\n",
      "Epoch 27/30\n",
      "158288/158288 [==============================] - 905s 6ms/sample - loss: 4.1566 - acc: 0.2236\n",
      "Epoch 28/30\n",
      "158288/158288 [==============================] - 918s 6ms/sample - loss: 4.1014 - acc: 0.2289\n",
      "Epoch 29/30\n",
      "158288/158288 [==============================] - 932s 6ms/sample - loss: 4.0468 - acc: 0.2340\n",
      "Epoch 30/30\n",
      "158288/158288 [==============================] - 935s 6ms/sample - loss: 3.9978 - acc: 0.2393\n"
     ]
    }
   ],
   "source": [
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#fit model\n",
    "model.fit(x,y,batch_size=64,epochs=30)\n",
    "#save the model to file\n",
    "model.save('model.h5')\n",
    "#save the tokenizer\n",
    "pickle.dump(tokenizer,open('tokenizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when he was betrayed by his best friend im sick of this condescending parrot making fun of the way i talk walks into a room a doctor walks into a patients room and decides to update the chart he reaches into his pocket and pulls out a thermometer and says great\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_seq(model,tokenizer,seq_length,seed_text,n_words):\n",
    "    result=list()\n",
    "    in_text = seed_text\n",
    "    #generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        #encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        #truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded],maxlen=seq_length,truncating='pre')\n",
    "        #predict probabilities for each word\n",
    "        yhat=model.predict_classes(encoded, verbose=0)\n",
    "        #map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        #append ti input\n",
    "        in_text +=''+ out_word\n",
    "        result.append(out_word)\n",
    "    return ''.join(result)\n",
    "\n",
    "#load cleaned text sequences\n",
    "in_filename = 'scifi_sequences.txt'\n",
    "text = load_text(in_filename)\n",
    "lines = text.split('\\n')\n",
    "seq_length = len(lines[0].split())-1\n",
    "\n",
    "#load model\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "#load the tokenizer\n",
    "tokenizer =pickle.load(open('tokenizer.pkl','rb'))\n",
    "\n",
    "#select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text +'\\n')\n",
    "\n",
    "#generate new text\n",
    "generated = generate_seq(model,tokenizer,seq_length, seed_text, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
